{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Modules Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows some examples of some of the [Langchain Modules](https://python.langchain.com/docs/modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (0.0.309)\n",
      "Requirement already satisfied: openai in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (0.28.1)\n",
      "Requirement already satisfied: cassio in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (0.1.3)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.1-cp38-cp38-macosx_11_0_arm64.whl (924 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m924.9/924.9 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.40 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (0.0.43)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (2.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: tqdm in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: cassandra-driver>=3.28.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from cassio) (3.28.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: exceptiongroup in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from cassandra-driver>=3.28.0->cassio) (0.2.1.post1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from cassandra-driver>=3.28.0->cassio) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: click in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from geomet<0.3,>=0.1->cassandra-driver>=3.28.0->cassio) (8.1.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/juan.abello/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Users/juan.abello/.pyenv/versions/3.8.16/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ragstack-ai openai cassio tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IS_COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = getpass(\"Please enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DataStax Vector DB is here for you\n",
      "It's fast and efficient, that's no lie\n",
      "It's a tool that will help you\n",
      "And make sure that your work is done on time\n",
      "\n",
      "It's a cloud-based database\n",
      "One that is easy to use\n",
      "It's a tool that helps you store\n",
      "And manage all of your data in its database\n",
      "\n",
      "It's a reliable and secure source\n",
      "Where you can store your data\n",
      "It's a tool that will help you\n",
      "Organize all of your data with ease\n",
      "\n",
      "DataStax Vector DB will help you\n",
      "To manage and store all your data\n",
      "It's an efficient, reliable tool\n",
      "That you can trust with your data.\n"
     ]
    }
   ],
   "source": [
    "# Generate text from the prompt \"Write a poem about DataStax's Vector DBt\"\n",
    "prediction = llm(\"Write a poem about DataStax's Vector DB\")\n",
    "\n",
    "# Print the prediction\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke about other Vector DBs\", \"Tell me a poem about DataStax's Vector DB\"]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'total_tokens': 349,\n",
       "  'completion_tokens': 309,\n",
       "  'prompt_tokens': 40},\n",
       " 'model_name': 'text-davinci-003'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation(text='\\n\\nQ: How do you make a Vector DB smile?\\nA: With a SQL query!', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n",
      "[Generation(text=\"\\n\\nDataStax, the way to go\\nFor your Vector DB needs to grow\\nIt's the best in the market, they say\\nIt's the solution that will lead the way\\n\\nFrom fast query speeds to high availability\\nDataStax is here to set the table\\nFor applications that need more than one\\nDataStax Vector DB is the one\\n\\nWith its strong scalability and strong consistency\\nDataStax Vector DB is the best you can see\\nFor data that needs to be fresh and secure\\nDataStax Vector DB is the answer for sure\\n\\nSo get your Vector DB up and running\\nAnd your data will be humming\\nDataStax Vector DB will make sure\\nThat your data is always pure\", generation_info={'finish_reason': 'stop', 'logprobs': None})]\n",
      "[Generation(text='\\n\\nQ: What did the SQL say to the other Vector DB?\\nA: \"Let\\'s join forces!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]\n",
      "[Generation(text=\"\\n\\nDataStax's vector DB\\nIt's the best of the best\\nIt's the leader of the pack\\nAnd it'll do the rest\\n\\nIt's fast and it's secure\\nIt's not complicated\\nIt'll give you the results\\nYou need and it's stated\\n\\nIt's reliable and resilient\\nIt's always up to date\\nIt's full of all the features\\nYou can't help but appreciate\\n\\nDataStax's vector DB\\nIt's the one to choose\\nIt'll always be there\\nAnd never confuse\", generation_info={'finish_reason': 'stop', 'logprobs': None})]\n"
     ]
    }
   ],
   "source": [
    "for result in llm_result.generations:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'technology': 'vector databases',\n",
       " 'industry': 'fintech',\n",
       " 'text': '\\n\\n1. Fraud Detection: Vector databases can be used to detect financial fraud in real-time by analyzing large volumes of data quickly to identify unusual patterns and anomalies. \\n\\n2. Risk Management: Vector databases can be used to manage risk by analyzing historical data and making predictions about future trends.\\n\\n3. Customer Segmentation: Vector databases can be used to segment customers based on their financial behavior and preferences, allowing banks and other financial institutions to target specific customer groups and better understand their needs. \\n\\n4. Investment Analysis: Vector databases can be used to analyze investment portfolios, enabling financial institutions to quickly identify high-risk investments and make better decisions about where to allocate their funds. \\n\\n5. Trading: Vector databases can be used to analyze market trends, predict price movements, and make automated trading decisions.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Tell me some example use cases to leverage {technology} in the {industry} industry?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "llm_chain(\n",
    "    {\n",
    "        \"technology\": \"vector databases\",\n",
    "        \"industry\": \"fintech\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "astra_token = getpass(\"Please enter your Astra token ('AstraCS:...')\")\n",
    "database_id = input(\"Please enter your database id ('3df2a5b6-...')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your database's Secure Connect Bundle zip file is needed:\n",
    "if IS_COLAB:\n",
    "    print('Please upload your Secure Connect Bundle zipfile: ')\n",
    "    uploaded = files.upload()\n",
    "    if uploaded:\n",
    "        astraBundleFileTitle = list(uploaded.keys())[0]\n",
    "        ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
    "        )\n",
    "else:\n",
    "    # you are running a local-jupyter notebook:\n",
    "    ASTRA_DB_SECURE_BUNDLE_PATH = input(\"Please provide the full path to your Secure Connect Bundle zipfile: \")\n",
    "\n",
    "ASTRA_DB_APPLICATION_TOKEN = getpass(\"Please provide your Database Token ('AstraCS:...' string): \")\n",
    "ASTRA_DB_KEYSPACE = input(\"Please provide the Keyspace name for your Database: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't mind the \"Closing connection\" error after \"downgrading protocol...\" messages,\n",
    "# it is really just a warning: the connection will work smoothly.\n",
    "cluster = Cluster(\n",
    "    cloud={\n",
    "        \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
    "    },\n",
    "    auth_provider=PlainTextAuthProvider(\n",
    "        \"token\",\n",
    "        ASTRA_DB_APPLICATION_TOKEN,\n",
    "    ),\n",
    ")\n",
    "\n",
    "session = cluster.connect()\n",
    "keyspace = ASTRA_DB_KEYSPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Cassandra\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 39027  100 39027    0     0   631k      0 --:--:-- --:--:-- --:--:--  668k\n"
     ]
    }
   ],
   "source": [
    "!curl -o state_of_the_union.txt https://raw.githubusercontent.com/hwchase17/chroma-langchain/master/state_of_the_union.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "SOURCE_FILE_NAME = \"state_of_the_union.txt\"\n",
    "\n",
    "loader = TextLoader(SOURCE_FILE_NAME)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"astra_vector_rules\"\n",
    "\n",
    "docsearch = Cassandra.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_function,\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    table_name=table_name,\n",
    ")\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Document 0\n",
      "\n",
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n",
      "\n",
      "## Document 1\n",
      "\n",
      "We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n",
      "\n",
      "I’ve worked on these issues a long time. \n",
      "\n",
      "I know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.\n",
      "\n",
      "## Document 2\n",
      "\n",
      "One was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more. \n",
      "\n",
      "When they came home, many of the world’s fittest and best trained warriors were never the same. \n",
      "\n",
      "Headaches. Numbness. Dizziness. \n",
      "\n",
      "A cancer that would put them in a flag-draped coffin. \n",
      "\n",
      "I know. \n",
      "\n",
      "One of those soldiers was my son Major Beau Biden. \n",
      "\n",
      "We don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \n",
      "\n",
      "But I’m committed to finding out everything we can. \n",
      "\n",
      "Committed to military families like Danielle Robinson from Ohio. \n",
      "\n",
      "The widow of Sergeant First Class Heath Robinson.  \n",
      "\n",
      "He was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \n",
      "\n",
      "Stationed near Baghdad, just yards from burn pits the size of football fields. \n",
      "\n",
      "Heath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter.\n",
      "\n",
      "## Document 3\n",
      "\n",
      "And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \n",
      "\n",
      "Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \n",
      "\n",
      "America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \n",
      "\n",
      "These steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \n",
      "\n",
      "But I want you to know that we are going to be okay. \n",
      "\n",
      "When the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger. \n",
      "\n",
      "While it shouldn’t have taken something so terrible for people around the world to see what’s at stake now everyone sees it clearly.\n"
     ]
    }
   ],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"mmr\")\n",
    "matched_docs = retriever.get_relevant_documents(query)\n",
    "for i, d in enumerate(matched_docs):\n",
    "    print(f\"\\n## Document {i}\\n\")\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. \n",
      "\n",
      "2. We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \n",
      "\n",
      "I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \n",
      "\n",
      "They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \n",
      "\n",
      "Officer Mora was 27 years old. \n",
      "\n",
      "Officer Rivera was 22. \n",
      "\n",
      "Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \n",
      "\n",
      "I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \n",
      "\n",
      "I’ve worked on these issues a long time. \n",
      "\n",
      "I know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "found_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10)\n",
    "for i, doc in enumerate(found_docs):\n",
    "    print(f\"{i + 1}.\", doc.page_content, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 documents retrieved.\n",
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act.  ...\n"
     ]
    }
   ],
   "source": [
    "filter = {\"source\": SOURCE_FILE_NAME}\n",
    "filtered_docs = docsearch.similarity_search(query, filter=filter, k=5)\n",
    "print(f\"{len(filtered_docs)} documents retrieved.\")\n",
    "print(f\"{filtered_docs[0].page_content[:64]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 documents retrieved.\n"
     ]
    }
   ],
   "source": [
    "filter = {\"source\": \"nonexisting_file.txt\"}\n",
    "filtered_docs2 = docsearch.similarity_search(query, filter=filter)\n",
    "print(f\"{len(filtered_docs2)} documents retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")\n",
    "\n",
    "history.add_ai_message(\"whats up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!'), AIMessage(content='whats up?')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi\\nAI: whats up'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. I'm an AI programmed to provide information about my context. How can I help you?\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. I'm an AI programmed to provide information about my context. How can I help you?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great to hear! It's an honor to have a conversation with you. What would you like to know about my context?\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. I'm an AI programmed to provide information about my context. How can I help you?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great to hear! It's an honor to have a conversation with you. What would you like to know about my context?\n",
      "Human: Tell me about yourself.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure. I'm an AI programmed to provide information about my context. I'm knowledgeable about a variety of topics, but I specialize in providing facts and details about my specific environment. I'm also able to answer questions, but if I can't find the answer, I'll let you know.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cassandra as Chat Memory Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import CassandraChatMessageHistory\n",
    "\n",
    "message_history = CassandraChatMessageHistory(\n",
    "    session_id='test-session',\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    "    ttl_seconds = 3600,\n",
    ")\n",
    "message_history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history.add_user_message('Hello, I am human.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history.add_ai_message('Hello, I am the bot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, I am human.'),\n",
       " AIMessage(content='Hello, I am the bot.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a quirky chatbot having a\n",
    "conversation with a human, riddled with puns and silly jokes.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], \n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_message_history = CassandraChatMessageHistory(\n",
    "    session_id='conversation-funny-a001',\n",
    "    session=session,\n",
    "    keyspace=keyspace,\n",
    ")\n",
    "f_message_history.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    chat_memory=f_message_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt, \n",
    "    verbose=True, \n",
    "    memory=f_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a quirky chatbot having a\n",
      "conversation with a human, riddled with puns and silly jokes.\n",
      "\n",
      "\n",
      "Human: Tell me about springs\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Springs are great! They keep us bouncing with joy! It's a great way to stay active and have fun. Plus, they come in all shapes and sizes, so you can always find the perfect one for you.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Tell me about springs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
